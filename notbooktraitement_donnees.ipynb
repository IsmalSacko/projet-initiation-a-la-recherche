{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "def traitement_document_gallica(url):\n",
    "    try:\n",
    "        # Récupération du contenu HTML du document\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "        \n",
    "        # Suppression des balises HTML\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        clean_text = soup.get_text()\n",
    "        \n",
    "        # Chargement du modèle de langue française\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        \n",
    "        # Traitement NER\n",
    "        doc = nlp(clean_text)\n",
    "        \n",
    "        # Affichage des entités nommées détectées\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)} ) ({spacy.displacy.render(doc, style='ent')})\")\n",
    "        \n",
    "        return doc  # Vous pouvez également retourner le doc ou d'autres données selon vos besoins\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(\"Erreur lors de la requête :\", str(e))\n",
    "        return None\n",
    "\n",
    "# Appel de la méthode pour traiter le document Gallica avec l'URL spécifié\n",
    "url_gallica = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "traitement_document_gallica(url_gallica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour normaliser les entités nommées détectées dans un document Gallica\n",
    "def normaliser_entites_nommees(url):\n",
    "    try:\n",
    "        # Récupération du contenu HTML du document\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "        \n",
    "        # Suppression des balises HTML\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        clean_text = soup.get_text()\n",
    "        \n",
    "        # Chargement du modèle de langue française\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        \n",
    "        # Traitement NER\n",
    "        doc = nlp(clean_text)\n",
    "        \n",
    "        # Normalisation des entités nommées détectées\n",
    "        entites_normalisees = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"LOC\":  # Exemple : normalisation des lieux\n",
    "                entite_normalisee = normaliser_lieux(ent.text)\n",
    "                entites_normalisees.append((entite_normalisee, ent.label_))\n",
    "            elif ent.label_ == \"DATE\":  # Exemple : normalisation des dates\n",
    "                entite_normalisee = normaliser_dates(ent.text)\n",
    "                entites_normalisees.append((entite_normalisee, ent.label_))\n",
    "            elif ent.label_ == \"PER\":  # Exemple : normalisation des personnes\n",
    "                entite_normalisee = normaliser_personnes(ent.text)\n",
    "                entites_normalisees.append((entite_normalisee, ent.label_))    \n",
    "            else:\n",
    "                entites_normalisees.append((ent.text, ent.label_))\n",
    "        \n",
    "        return entites_normalisees\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(\"Erreur lors de la requête :\", str(e))\n",
    "        return None\n",
    "\n",
    "# Fonctions de normalisation des lieux\n",
    "def normaliser_lieux(entite):\n",
    "    # Par exemple, convertir en majuscules, supprimer les espaces, etc.\n",
    "    return entite.upper().replace(\"  \", \" \")\n",
    "\n",
    "# Fonctions de normalisation des dates\n",
    "def normaliser_dates(entite):\n",
    "   return entite\n",
    "def normaliser_personnes(entite):\n",
    "    # convertir en majuscules, supprimer les espaces en trop, etc.\n",
    "    return entite.upper().replace(\"  \", \" \")\n",
    "    \n",
    "# Appel de la méthode de normalisation des entités nommées pour l'URL spécifiée\n",
    "url_gallica = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "entites_normalisees = normaliser_entites_nommees(url_gallica)\n",
    "\n",
    "# Affichage des entités nommées normalisées\n",
    "if entites_normalisees:\n",
    "    print(\"Entités nommées normalisées:\")\n",
    "    for entite, label in entites_normalisees:\n",
    "        print(f\"{entite}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour inserer les entites nommées dans une base de données sqlite à parir la liste des entités nommées normalisées\n",
    "def inserer_entites_nommees(entites_normalisees):\n",
    "    try:\n",
    "        # Connexion à la base de données SQLite\n",
    "        conn = sqlite3.connect(\"entites.db\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Création de la table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS entites_nommees (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "                entite TEXT,\n",
    "                label TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Insertion des entités nommées dans la base de données\n",
    "        cursor.executemany(\"\"\"\n",
    "            INSERT INTO entites_nommees (entite, label) VALUES (?, ?)\n",
    "        \"\"\", entites_normalisees)\n",
    "        \n",
    "        # Sauvegarde de la transaction\n",
    "        conn.commit()\n",
    "        \n",
    "        # Fermeture de la connexion à la base de données\n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Erreur lors de la requête SQL :\", str(e))\n",
    "        return None\n",
    "# Appel de la méthode pour insérer les entités nommées dans la base de données\n",
    "inserer_entites_nommees(entites_normalisees) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocker les entités nommées normalisées dans un DataFrame\n",
    "df = pd.DataFrame(entites_normalisees, columns=[\"entite\", \"label\"])\n",
    "df.head()\n",
    "# exporter le DataFrame au format CSV\n",
    "entite_csv = df.to_csv(\"entites_normalisees.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocker les entités nommées normalisées dans une base de données SQLite sans utiliser une fonction\n",
    "conn = sqlite3.connect(\"entites_normalisees1.db\")\n",
    "df.to_sql(\"entites_normalisees\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def preparer_donnees_classification(url):\n",
    "    try:\n",
    "        # Récupération du contenu HTML du document\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "        \n",
    "        # Suppression des balises HTML\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        clean_text = soup.get_text()\n",
    "        \n",
    "        # Chargement du modèle de langue française\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        \n",
    "        # Traitement NER\n",
    "        doc = nlp(clean_text)\n",
    "\n",
    "        data = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['PER', 'ORG']:  # Filtrer les entités de type Personne (PER) ou Organisation (ORG)\n",
    "                data.append((ent.text, ent.label_))\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['Texte', 'Label'])\n",
    "        df['Label'] = df['Label'].map({'PER': 0, 'ORG': 1})\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df['Texte'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "\n",
    "        return model, vectorizer\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(\"Erreur lors de la requête :\", str(e))\n",
    "        return None\n",
    "\n",
    "# Utilisation de l'URL url_gallica dans la fonction de préparation des données pour la classification\n",
    "url_gallica = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "model, vectorizer = preparer_donnees_classification(url_gallica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catégorie de l'entité : Catégorie indéterminée\n"
     ]
    }
   ],
   "source": [
    "# Utilisation du modèle pour la classification\n",
    "\"\"\"\n",
    "Dans cet exemple, la fonction classifier_entites_nommees prend le modèle entraîné et le vectorizer\n",
    " (qui ont été obtenus à partir de la préparation des données) ainsi qu'un texte à classifier. \n",
    " Cette fonction vectorise le texte en utilisant le vectorizer, puis utilise le modèle pour prédire \n",
    " la catégorie de l'entité dans le texte donné.\n",
    "\"\"\"\n",
    "def classifier_entites_nommees(model, vectorizer, texte):\n",
    "    # Vectorisation du texte avec le vectorizer\n",
    "    texte_vectorise = vectorizer.transform([texte])\n",
    "    \n",
    "    # Prédiction de la catégorie de l'entité\n",
    "    prediction = model.predict(texte_vectorise)\n",
    "    \n",
    "    # Interprétation de la prédiction\n",
    "    if prediction[0] == 0:\n",
    "        return \"Personne\"\n",
    "    elif prediction[0] == 1:\n",
    "        return \"Organisation\"\n",
    "    else:\n",
    "        return \"Catégorie indéterminée\"\n",
    "\n",
    "# Texte à classifier\n",
    "texte_a_classifier = \"Texte à classer, extrait du document Gallica...\"\n",
    "\n",
    "# Utilisation du modèle pour classifier le texte\n",
    "categorie = classifier_entites_nommees(model, vectorizer, texte_a_classifier)\n",
    "print(f\"Catégorie de l'entité : {categorie}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on compare les résultats de SpaCy et Flair pour le français sur le même texte\n",
    "pour les entités de type Personne (PER) et Organisation (ORG).\n",
    "Cela utilise la bibliothèque Flair pour extraire les entités nommées du texte, puis les compare avec les entités extraites par SpaCy pour permettre une comparaison visuelle. Vous pouvez ajuster les paramètres selon vos besoins spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# URL du document sur Gallica\n",
    "url = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Supprimer les balises HTML\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "clean_text = soup.get_text()\n",
    "\n",
    "# Modèle de langue française avec réseaux neuronaux de SpaCy\n",
    "nlp_spacy = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Traitement NER avec SpaCy\n",
    "doc_spacy = nlp_spacy(clean_text)\n",
    "entities_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents if ent.label_ in ['PER', 'ORG']]\n",
    "\n",
    "# Traitement NER avec Flair\n",
    "tagger = SequenceTagger.load('ner-ontonotes')\n",
    "sentence = Sentence(clean_text)\n",
    "tagger.predict(sentence)\n",
    "entities_flair = [(entity.text, entity.get_labels()[0].value) for entity in sentence.get_spans('ner')]\n",
    "\n",
    "# Stockage des entités nommées dans des DataFrames\n",
    "df_spacy = pd.DataFrame(entities_spacy, columns=['Entité', 'Type (SpaCy)'])\n",
    "df_flair = pd.DataFrame(entities_flair, columns=['Entité', 'Type (Flair)'])\n",
    "\n",
    "# Affichage des DataFrames pour comparaison\n",
    "print(\"Résultats de SpaCy:\")\n",
    "print(df_spacy.head())  # Affiche les 5 premières entités de SpaCy\n",
    "print(\"\\nRésultats de Flair:\")\n",
    "print(df_flair.head())  # Affiche les 5 premières entités de Flair\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
