{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce script, nous allons voir comment utiliser le module spaCy pour détecter les entités nommées dans un document Gallica via l'API de Gallica.\n",
    "On va utiliser le modèle de langue française de spaCy pour détecter les entités nommées dans le document.\n",
    "On va aussi utiliser BeautifulSoup pour supprimer les balises HTML du document et récupérer le texte brut pour les traitements NLP.\n",
    "Nous allons également utiliser le module requests pour récupérer le contenu HTML du document Gallica.\n",
    "Et après on les nettoie avec BeautifulSoup pour afficher les entités nommées détectées et sqlite3 pour stocker les entités nommées dans une base de données SQLite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "def traitement_document_gallica(url):\n",
    "    try:\n",
    "        # Récupération du contenu HTML du document\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "        \n",
    "        # Suppression des balises HTML\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        clean_text = soup.get_text()\n",
    "        \n",
    "        # Chargement du modèle de langue française\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        \n",
    "        # Traitement NER\n",
    "        doc = nlp(clean_text)\n",
    "        \n",
    "        # Affichage des entités nommées détectées\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)} ) \")\n",
    "        \n",
    "        return doc  # Vous pouvez également retourner le doc ou d'autres données selon vos besoins\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(\"Erreur lors de la requête :\", str(e))\n",
    "        return None\n",
    "\n",
    "# Appel de la méthode pour traiter le document Gallica avec l'URL spécifié\n",
    "url_gallica = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "traitement_document_gallica(url_gallica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ici nous normalisons le texte pour le rendre plus lisible et plus facile à traiter.\n",
    "On va assi normaliser les entités nommées pour les rendre plus facilement identifiables (par exemple, on va remplacer les dates par la chaîne de caractères \"DATE\").\n",
    "Remplacer les espaces multiples par un seul espace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entités nommées normalisées:\n",
      "ALFRED BONZON: PER\n",
      "Bourse de Lyon: ORG\n",
      "GALLICA                                                              : PER\n",
      "FR   EN   DE   ES: MISC\n",
      "IT   RU                                        : MISC\n",
      "BIBLIOTHÈQUE NATIONALE DE FRANCE   : LOC\n",
      "GALLICA  : PER\n",
      "GALLICA: MISC\n",
      "SUGGÉRÉS: MISC\n",
      "HUGO: PER\n",
      "VICTOR HUGO: PER\n",
      "VICTOR-MARIE ATELIERS: PER\n",
      "ALÉSI HUGO: PER\n",
      "FRANÇOIS-VICTOR HUGO: PER\n",
      "ABEL HUGO: PER\n",
      "CHARLES : PER\n",
      "HUGO DE SANCTO: PER\n",
      "HUGO: MISC\n",
      "VICTORE: MISC\n",
      "HUGO: PER\n",
      "JOSEPH: PER\n",
      "SUGGÉRÉS: MISC\n",
      "HUGO DE SANCTO: PER\n",
      "LOUIS : PER\n",
      "Gentilly  : MISC\n",
      "HUGO GROTIUS: PER\n",
      "GEBIETE: PER\n",
      "Victor: MISC\n",
      "HUGO DE SANCTO: PER\n",
      "GÉNÉRAL HUGO: PER\n",
      "PAPE: PER\n",
      "VICTOR HUGO: PER\n",
      "L’EGLISE     RECHERCHE: PER\n",
      "ACCÉDER: LOC\n",
      "Recherche avancée     : MISC\n",
      "Sélections Accéder: ORG\n",
      "ACCÉDER: LOC\n",
      "TYPES: LOC\n",
      "Livres   Manuscrits   Cartes   Images   Presse: MISC\n",
      "Enregistrements sonores   : MISC\n",
      "Partitions   : MISC\n",
      "Vidéos     Thématiques   Arts: ORG\n",
      "Histoire   Langues   Littératures   Philosophie   : MISC\n",
      "Panier   Panier     : MISC\n",
      "Espace personnel       : ORG\n",
      "AIDE: PER\n",
      "Langue du site Langue du site       Français: MISC\n",
      "English-EN       : MISC\n",
      "GALLICA: MISC\n",
      "Rechercher        : MISC\n",
      "Sélections Accéder: ORG\n",
      "TYPES: LOC\n",
      "Livres   Manuscrits   Cartes   Images   Presse: MISC\n",
      "Enregistrements sonores   : MISC\n",
      "Partitions   : MISC\n",
      "Vidéos     Thématiques   Arts: ORG\n",
      "Histoire   Langues   Littératures   Philosophie   : MISC\n",
      "SAVOIR PLUS: MISC\n",
      "DÉCOUVRIR                                                                                                                                                                                                                                                        : ORG\n"
     ]
    }
   ],
   "source": [
    "# fonction pour normaliser les entités nommées détectées dans un document Gallica\n",
    "def normaliser_entites_nommees(url):\n",
    "    try:\n",
    "        # Récupération du contenu HTML du document\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "        \n",
    "        # Suppression des balises HTML\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        clean_text = soup.get_text()\n",
    "        \n",
    "        # Chargement du modèle de langue française\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        \n",
    "        # Traitement NER\n",
    "        doc = nlp(clean_text)\n",
    "        \n",
    "        # Normalisation des entités nommées détectées\n",
    "        entites_normalisees = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"LOC\":  # Exemple : normalisation des lieux\n",
    "                entite_normalisee = normaliser_lieux(ent.text)\n",
    "                entites_normalisees.append((entite_normalisee, ent.label_))\n",
    "            elif ent.label_ == \"DATE\":  # Exemple : normalisation des dates\n",
    "                entite_normalisee = normaliser_dates(ent.text)\n",
    "                entites_normalisees.append((entite_normalisee, ent.label_))\n",
    "            elif ent.label_ == \"PER\":  # Exemple : normalisation des personnes\n",
    "                entite_normalisee = normaliser_personnes(ent.text)\n",
    "                entites_normalisees.append((entite_normalisee, ent.label_))    \n",
    "            else:\n",
    "                entites_normalisees.append((ent.text, ent.label_))\n",
    "        \n",
    "        return entites_normalisees\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(\"Erreur lors de la requête :\", str(e))\n",
    "        return None\n",
    "\n",
    "# Fonctions de normalisation des lieux\n",
    "def normaliser_lieux(entite):\n",
    "    # Par exemple, convertir en majuscules, supprimer les espaces, etc.\n",
    "    return entite.upper().replace(\"  \", \" \")\n",
    "\n",
    "# Fonctions de normalisation des dates\n",
    "def normaliser_dates(entite):\n",
    "   return entite\n",
    "def normaliser_personnes(entite):\n",
    "    # convertir en majuscules, supprimer les espaces en trop, etc.\n",
    "    return entite.upper().replace(\"  \", \" \")\n",
    "    \n",
    "# Appel de la méthode de normalisation des entités nommées pour l'URL spécifiée\n",
    "url_gallica = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "entites_normalisees = normaliser_entites_nommees(url_gallica)\n",
    "\n",
    "# Affichage des entités nommées normalisées\n",
    "if entites_normalisees:\n",
    "    print(\"Entités nommées normalisées:\")\n",
    "    for entite, label in entites_normalisees:\n",
    "        print(f\"{entite}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir normalisé le texte, nous inserons les entités nommées détectées dans une base de données SQLite.\n",
    "Ou encore les stocker dans un fichier CSV. Cette technique est utile pour l'analyse de corpus et rendra les données reutilisables pour des chercheurs en humanités numériques nou en histoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour inserer les entites nommées dans une base de données sqlite à parir la liste des entités nommées normalisées\n",
    "def inserer_entites_nommees(entites_normalisees):\n",
    "    try:\n",
    "        # Connexion à la base de données SQLite\n",
    "        conn = sqlite3.connect(\"entites.db\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Création de la table\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS entites_nommees (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "                entite TEXT,\n",
    "                label TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Insertion des entités nommées dans la base de données\n",
    "        cursor.executemany(\"\"\"\n",
    "            INSERT INTO entites_nommees (entite, label) VALUES (?, ?)\n",
    "        \"\"\", entites_normalisees)\n",
    "        \n",
    "        # Sauvegarde de la transaction\n",
    "        conn.commit()\n",
    "        \n",
    "        # Fermeture de la connexion à la base de données\n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Erreur lors de la requête SQL :\", str(e))\n",
    "        return None\n",
    "# Appel de la méthode pour insérer les entités nommées dans la base de données\n",
    "inserer_entites_nommees(entites_normalisees) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partir permet de céer un data frame avec les entités nommées détectées et les afficher dans un tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocker les entités nommées normalisées dans un DataFrame\n",
    "df = pd.DataFrame(entites_normalisees, columns=[\"entite\", \"label\"])\n",
    "df.head()\n",
    "# exporter le DataFrame au format CSV\n",
    "entite_csv = df.to_csv(\"entites_normalisees.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On stocke les entités nommées normalisées dans une base de données SQLite, sans passer par un DataFrame ou une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocker les entités nommées normalisées dans une base de données SQLite sans utiliser une fonction\n",
    "conn = sqlite3.connect(\"entites_normalisees1.db\")\n",
    "df.to_sql(\"entites_normalisees\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catégorie de l'entité : Catégorie indéterminée\n"
     ]
    }
   ],
   "source": [
    "# Utilisation du modèle pour la classification\n",
    "\"\"\"\n",
    "Dans cet exemple, la fonction classifier_entites_nommees prend le modèle entraîné et le vectorizer\n",
    " (qui ont été obtenus à partir de la préparation des données) ainsi qu'un texte à classifier. \n",
    " Cette fonction vectorise le texte en utilisant le vectorizer, puis utilise le modèle pour prédire \n",
    " la catégorie de l'entité dans le texte donné.\n",
    "\"\"\"\n",
    "def classifier_entites_nommees(model, vectorizer, texte):\n",
    "    # Vectorisation du texte avec le vectorizer\n",
    "    texte_vectorise = vectorizer.transform([texte])\n",
    "    \n",
    "    # Prédiction de la catégorie de l'entité\n",
    "    prediction = model.predict(texte_vectorise)\n",
    "    \n",
    "    # Interprétation de la prédiction\n",
    "    if prediction[0] == 0:\n",
    "        return \"Personne\"\n",
    "    elif prediction[0] == 1:\n",
    "        return \"Organisation\"\n",
    "    else:\n",
    "        return \"Catégorie indéterminée\"\n",
    "\n",
    "# Texte à classifier\n",
    "texte_a_classifier = \"Texte à classer, extrait du document Gallica...\"\n",
    "\n",
    "# Utilisation du modèle pour classifier le texte\n",
    "categorie = classifier_entites_nommees(model, vectorizer, texte_a_classifier)\n",
    "print(f\"Catégorie de l'entité : {categorie}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on compare les résultats de SpaCy et Flair pour le français sur le même texte\n",
    "pour les entités de type Personne (PER) et Organisation (ORG).\n",
    "Cela utilise la bibliothèque Flair pour extraire les entités nommées du texte, puis les compare avec les entités extraites par SpaCy pour permettre une comparaison visuelle. Vous pouvez ajuster les paramètres selon vos besoins spécifiques.\n",
    "\n",
    "On pour décider par la suite quelle méthode utiliser pour détecter les entités nommées dans les documents Gallica avec moins d'erreurs possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ismal\\projets\\initiation_a_la_recherche\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ismal\\projets\\initiation_a_la_recherche\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2023-12-26 17:00:25,939 SequenceTagger predicts: Dictionary with 75 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-GPE, B-GPE, E-GPE, I-GPE, S-ORG, B-ORG, E-ORG, I-ORG, S-DATE, B-DATE, E-DATE, I-DATE, S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-NORP, B-NORP, E-NORP, I-NORP, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL, S-LOC, B-LOC, E-LOC, I-LOC, S-TIME, B-TIME, E-TIME, I-TIME, S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART, S-FAC\n",
      "Résultats de SpaCy:\n",
      "                                              Entité Type (SpaCy)\n",
      "0                                      Alfred Bonzon          PER\n",
      "1                                     Bourse de Lyon          ORG\n",
      "2  Gallica                                       ...          PER\n",
      "3                                         Gallica             PER\n",
      "4                                               Hugo          PER\n",
      "\n",
      "Résultats de Flair:\n",
      "                   Entité Type (Flair)\n",
      "0          Bourse de Lyon          FAC\n",
      "1                    Hugo       PERSON\n",
      "2            Victor  Hugo       PERSON\n",
      "3  Victor-Marie  Ateliers       PERSON\n",
      "4      Hugo d’Alési  Hugo       PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# URL du document sur Gallica\n",
    "url = \"https://gallica.bnf.fr/ark:/12148/bpt6k1426047z\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Supprimer les balises HTML\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "clean_text = soup.get_text()\n",
    "\n",
    "# Modèle de langue française avec réseaux neuronaux de SpaCy\n",
    "nlp_spacy = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Traitement NER avec SpaCy\n",
    "doc_spacy = nlp_spacy(clean_text)\n",
    "entities_spacy = [(ent.text, ent.label_) for ent in doc_spacy.ents if ent.label_ in ['PER', 'ORG']]\n",
    "\n",
    "# Traitement NER avec Flair\n",
    "tagger = SequenceTagger.load('ner-ontonotes')\n",
    "sentence = Sentence(clean_text)\n",
    "tagger.predict(sentence)\n",
    "entities_flair = [(entity.text, entity.get_labels()[0].value) for entity in sentence.get_spans('ner')]\n",
    "\n",
    "# Stockage des entités nommées dans des DataFrames\n",
    "df_spacy = pd.DataFrame(entities_spacy, columns=['Entité', 'Type (SpaCy)'])\n",
    "df_flair = pd.DataFrame(entities_flair, columns=['Entité', 'Type (Flair)'])\n",
    "\n",
    "# Affichage des DataFrames pour comparaison\n",
    "print(\"Résultats de SpaCy:\")\n",
    "print(df_spacy.head())  # Affiche les 5 premières entités de SpaCy\n",
    "print(\"\\nRésultats de Flair:\")\n",
    "print(df_flair.head())  # Affiche les 5 premières entités de Flair\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
